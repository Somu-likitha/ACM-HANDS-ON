{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "&emsp; Transformers, a game-changing development in machine learning, emerged in 2017 with the paper **\"Attention Is All You Need\"** by Vaswani et al. These models have notably impacted the field, excelling in natural language processing **(NLP)** tasks like **translation**, **sentiment analysis**, and **text summarization**. Famous Transformers include **BERT**, **GPT-3**, and **RoBERTa**.\n",
    "\n",
    "Here are some of the key features of Transformers that make them so powerful:\n",
    "\n",
    "- Transformers effectively **capture long-range correlation** in the data, a challenge for RNNs.\n",
    "- They employ **self-attention mechanisms** to process and contextualize information across the entire sequence.\n",
    "- Unlike RNNs, Transformers are **parallelizable** and thus more efficient to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from mnist import Train, Val\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from time import time, sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "repeat = 1_000\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(256, 28, 28).to(DEVICE)\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts and Components\n",
    "\n",
    "&emsp; Transformers are composed of several key components, including **self-attention**, **multi-head attention**, **masked attention**, **positional encoding** etc. These components are stacked together to form the Transformer architecture.\n",
    "\n",
    "<!-- ![Transformer Architecture](trying_to_make_transformer.svg) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Self-Attention Mechanism\n",
    "\n",
    "Self-attention is the basic building block of transformer models. Let us first understand the concept of self-attention without any learnable weights.\n",
    "\n",
    "&emsp; Let us take the example of some NLP task to understand the concept of self-attention. Let us have a sentences of length $n$. Also for each word in the sentence, we have a vector of length $k$. So this sentence can be represented as a matrix of size ($n \\times k$). Let us call this matrix $X_{n \\times k}$. <span style=\"font-size: 12px\">(we are not considering the batch dimension for simplicity)</span>\n",
    "\n",
    "#### Step 1:\n",
    "\n",
    "&emsp; We multiply $X$ with its transpose to get a matrix of size ($n \\times n$). Let us call this matrix $W_{n\\times n}$. This operation can be thought of as a similarity measure between each word in the sentence with every other word in the sentence. The diagonal elements of $W_{n\\times n}$ will be the similarity of each word with itself. The off-diagonal elements will be the similarity of each word with every other word in the sentence.\n",
    "\n",
    "$$\n",
    "W_{n\\times n} := X_{n \\times k} \\times X_{k \\times n}^T\n",
    "$$\n",
    "\n",
    "**How come the product of the two word embeddings gives the similarity between the two words?** (cosine similarity is equal to the dot product of two unit vectors)\n",
    "\n",
    "\n",
    "\n",
    "#### Step 2:\n",
    "\n",
    "&emsp; In this step, we scale the result by $\\frac{1}{\\sqrt{k}}$ ([why?](error_proof.ipynb)), and  apply a softmax function to each column of $W_{n\\times n}$. This step can be thought of as a way to normalize the similarity scores. After completing this step, we get an updated $W_{n\\times n}$ matrix.\n",
    "\n",
    "$$\n",
    "W_{n\\times n} := \\text{softmax}\\left(\\frac{W_{n\\times n}}{\\sqrt{k}}\\right)\n",
    "$$\n",
    "\n",
    "#### Step 3:\n",
    "\n",
    "&emsp; In this step, we multiply the updated $W_{n\\times n}$ with $X_{n \\times k}$ to get a matrix of size ($n \\times k$). Let us call this matrix $Y_{n \\times k}$. This step can be thought of as a weighted sum of the vectors in $X_{n \\times k}$, where the weights are the values in $W_{n\\times n}$. Thus as the new matrix $Y_{n \\times k}$, we have a matrix which has the same shape as $X_{n \\times k}$, but instead of telling about each word in the sentence, it tells about the importance of each word in the sentence in the context of every other word in the sentence.\n",
    "\n",
    "$$\n",
    "Y_{n \\times k} := W_{n\\times n} \\times X_{n \\times k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # size of x is expected to be [batch_size, seq_len=n, embedding_dim=k]\n",
    "        w = x @ x.transpose(-1, -2)  # w.size() = [batch_size, n, n]\n",
    "        w = w / torch.sqrt(torch.tensor(x.shape[-1]).float())  # w / sqrt(k)\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        y = w @ x  # y.size() = [batch_size, n, k]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention with Weights\n",
    "\n",
    "Now, as we know the concept of self Attention, we can extend it to self-attention with weights. In this case, we have three weight matrices $W^Q$, $W^K$, and $W^V$ all of shapes ($k \\times d$). Here $d$ is the output dimension. The next calculation is similar to the previous one and is shown below.\n",
    "\n",
    "$Q_{n \\times d} := X_{n \\times k} \\times W_{k \\times d}^Q$<br>\n",
    "$K_{n \\times d} := X_{n \\times k} \\times W_{k \\times d}^K$<br>\n",
    "$V_{n \\times d} := X_{n \\times k} \\times W_{k \\times d}^V$\n",
    "\n",
    "$W_{n\\times n}  := Q_{n\\times d} \\times K_{d \\times n}^T$<br>\n",
    "$W_{n\\times n}  := \\text{softmax}\\left(\\frac{W_{n\\times n}}{\\sqrt{d}}\\right)$<br>\n",
    "$Y_{n \\times d} := W_{n\\times n} \\times V_{n \\times d}$\n",
    "\n",
    "Note that, in case of self-attention without weights, the shape of $Y$ remained same as $X$ both ($n \\times k$). But in case of self-attention with weights, the shape of $X$ is ($n \\times k$) and the shape of $Y$ is ($n \\times d$). It is as if, the embedding dimension of each word in the sentence has been changed from $k$ to $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without using linear layers\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=None):\n",
    "        super().__init__()\n",
    "        if output_dim is None: output_dim = input_dim\n",
    "        Wq = torch.empty(input_dim, output_dim)\n",
    "        Wk = torch.empty(input_dim, output_dim)\n",
    "        Wv = torch.empty(input_dim, output_dim)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(Wq)\n",
    "        torch.nn.init.xavier_uniform_(Wk)\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "\n",
    "        self.Wq = nn.Parameter(Wq)\n",
    "        self.Wk = nn.Parameter(Wk)\n",
    "        self.Wv = nn.Parameter(Wv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # sixe of x is expected to be [batch_size, seq_len=n, input_dim=k]\n",
    "        Q = x @ self.Wq  # q.size() = [batch_size, n, output_dim=d]\n",
    "        K = x @ self.Wk  # k.size() = [batch_size, n, output_dim=d]\n",
    "        V = x @ self.Wv  # v.size() = [batch_size, n, output_dim=d]\n",
    "\n",
    "        w = Q @ K.transpose(-1, -2)  # w.size() = [batch_size, n, n]\n",
    "        w = w / torch.sqrt(torch.tensor(x.shape[-1]).float())\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        y = w @ V\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "In multi-head attention, we have multiple self-attention layers in parallel. Each of these layers has its own set of weights. The output of each of these layers is concatenated and then passed through a linear layer. The output of this linear layer is the final output of the multi-head attention layer. \n",
    "\n",
    "Suppose there are $h$ heads in the multi-head attention layer, and we take d as the output dimension for the multi-head attention layer. Incase we use $d$ as the output dimension for each head, the output from each heads will be of shape ($n \\times d$). The output of all these heads concatenated will be of shape ($n \\times d*h$). But we wanted the output of the multi-head attention layer to be of shape ($n \\times d$). So, instead of using $d$ as the output dimension for each head, we use $\\frac{d}{h}$ as the output dimension for each head. Thus the output of each head will be of shape ($n \\times \\frac{d}{h}$). The output of all these heads concatenated will be of shape ($n \\times d$). <span style=\"font-size: 12px\">(assuming h divides d perfectly)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert output_dim % num_heads == 0, f\"{output_dim = } must be divisible by {num_heads = }\"\n",
    "        self.head_dim = output_dim // num_heads\n",
    "\n",
    "        # nothing fancy, just initializing the weights\n",
    "        Wq = torch.empty(input_dim, output_dim)\n",
    "        Wk = torch.empty(input_dim, output_dim)\n",
    "        Wv = torch.empty(input_dim, output_dim)\n",
    "        torch.nn.init.xavier_uniform_(Wq)\n",
    "        torch.nn.init.xavier_uniform_(Wk)\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "        self.Wq = nn.Parameter(Wq)\n",
    "        self.Wk = nn.Parameter(Wk)\n",
    "        self.Wv = nn.Parameter(Wv)\n",
    "\n",
    "        self.fc_out = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = x @ self.Wq\n",
    "        K = x @ self.Wk\n",
    "        V = x @ self.Wv\n",
    "\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        w = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "        w = w / torch.sqrt(torch.tensor(self.head_dim).float())\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        y = torch.einsum(\"nhql,nlhd->nqhd\", [w, V])\n",
    "        y = y.reshape(batch_size, seq_len, -1)  # [batch_size, seq_len, output_dim]\n",
    "\n",
    "        out = self.fc_out(y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Attention\n",
    "\n",
    "As explained in step 2 of the vanilla self-attention mechanism, we scale the matrix $W_{n\\times n}$ by $\\frac{1}{\\sqrt{k}}$ and then apply a softmax function to each column of $W_{n\\times n}$. Here between scaling down the matrix and applying the softmax function, we do the masking.\n",
    "\n",
    "**Motivation:** In the case of language translation, we do not want the model to look at the words that come after the word that we are trying to translate. (For example, if we are trying to predict the next part of the sentence \"I am going to the\", we do not want the model to look at the words \"childrens park\" as it comes after the word \"the\". So we mask the words that come after the word that we are trying to predict. Now, in the next go, we will show it the word \"childrens\", and would expect it to predict just the word \"park\". So we mask the words that come after the word \"childrens\". This is called masking.)\n",
    "\n",
    "**How do we do masking?** We make the upper traiangular part of the matrix $W_{n\\times n}$ equal to $-\\infty$. This is done so that when we apply the softmax function, the values in the upper triangular part of the matrix become zero, and the the full probability mask is distributed only among the lower triangular part of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert output_dim % num_heads == 0, f\"{output_dim = } must be divisible by {num_heads = }\"\n",
    "        self.head_dim = output_dim // num_heads\n",
    "\n",
    "        # nothing fancy, just initializing the weights\n",
    "        Wq = torch.empty(input_dim, output_dim)\n",
    "        Wk = torch.empty(input_dim, output_dim)\n",
    "        Wv = torch.empty(input_dim, output_dim)\n",
    "        torch.nn.init.xavier_uniform_(Wq)\n",
    "        torch.nn.init.xavier_uniform_(Wk)\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "        self.Wq = nn.Parameter(Wq)\n",
    "        self.Wk = nn.Parameter(Wk)\n",
    "        self.Wv = nn.Parameter(Wv)\n",
    "\n",
    "        self.fc_out = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = x @ self.Wq\n",
    "        K = x @ self.Wk\n",
    "        V = x @ self.Wv\n",
    "\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        w = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "        w = w / torch.sqrt(torch.tensor(self.head_dim).float())\n",
    "\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)) == 0\n",
    "        mask = mask.to(self.Wq.device)\n",
    "        w = w.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        y = torch.einsum(\"nhql,nlhd->nqhd\", [w, V])\n",
    "        y = y.reshape(batch_size, seq_len, -1)  # [batch_size, seq_len, output_dim]\n",
    "\n",
    "        out = self.fc_out(y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Adhil has already explained this in detail, I am not going to repeat the whole here, but I will just give a brief overview of the concept.\n",
    "\n",
    "- Positional embedding vector is a vector of same shape as the input embeding vector and it's values don't change with the input.\n",
    "- It carries information about the position of the word in the sentence.\n",
    "- We element-wise add the positional embedding vector to the input embedding vector to get the final embedding vector for each word in the sentence.\n",
    "- Thus we get a new embedding vector which carries information about the word itself as well as its position in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "&emsp; We would build a small transformer model to understand the architecture of the transformer. The transformer model that we are going to build is shown below.\n",
    "\n",
    "<!-- ![Transformer Block](transformer_block.png) -->\n",
    "<img src=\"transformer_block.png\" alt=\"transformer block\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads=8, dropout=0.1, forward_expansion=1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, int(forward_expansion * embed_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(forward_expansion * embed_size), embed_size),\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.dropout(self.norm1(attention + x))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embed_size=28, num_classes=10, heads=4, forward_expansion=1, dropout=0.1):\n",
    "        super(Model, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        self.fc_out = nn.Linear(embed_size*embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.transformer_block(x)\n",
    "        out = out.flatten(start_dim=1)\n",
    "        out = self.fc_out(out)\n",
    "        return F.softmax(out, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
